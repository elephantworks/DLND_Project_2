{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile('cifar-10-python.tar.gz'):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            'cifar-10-python.tar.gz',\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open('cifar-10-python.tar.gz') as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 4:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1003, 1: 963, 2: 1041, 3: 976, 4: 1004, 5: 1021, 6: 1004, 7: 981, 8: 1024, 9: 983}\n",
      "First 20 Labels: [0, 6, 0, 2, 7, 2, 1, 2, 4, 1, 5, 6, 6, 3, 1, 3, 5, 5, 8, 1]\n",
      "\n",
      "Example of Image 693:\n",
      "Image - Min Value: 21 Max Value: 255\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 1 Name: automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njSWeMGcB8wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11ef988d0>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 4\n",
    "sample_id = 693\n",
    "\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return np.array(x / 255)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    x = np.array(x)\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    lb.fit(np.array([i for i in range(10)]))\n",
    "    return lb.transform(x)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a bach of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(dtype=tf.float32, shape=[None, image_shape[0], image_shape[1], image_shape[2]], name=\"x\")\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(dtype=tf.float32, shape=[None, n_classes], name=\"y\")\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(dtype=tf.float32, name=\"keep_prob\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    depth = x_tensor.get_shape().as_list()[3]\n",
    "    shape = [conv_ksize[0], conv_ksize[1], depth, conv_num_outputs] \n",
    "    weights = tf.Variable(tf.random_normal(shape, stddev = 0.1))\n",
    "    bias = tf.Variable(tf.zeros(conv_num_outputs))\n",
    "    strides = [1, conv_strides[0], conv_strides[1], 1]\n",
    "    conv = tf.nn.conv2d(x_tensor, weights, strides, padding='SAME') + bias\n",
    "    conv = tf.nn.relu(conv)\n",
    "    ksize = [1, pool_ksize[0], pool_ksize[1], 1]\n",
    "    strides = [1, pool_strides[0], pool_strides[1], 1]\n",
    "    return tf.nn.max_pool(conv, ksize, strides, padding='SAME')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.contrib.layers.flatten(x_tensor)\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.contrib.layers.fully_connected(x_tensor, num_outputs)\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.contrib.layers.fully_connected(x_tensor, num_outputs)\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    num_outputs = 10\n",
    "    conv_outputs = 96\n",
    "    conv_ksize = (8, 8)\n",
    "    conv_strides = (4, 4)\n",
    "    pool_ksize = (4, 4)\n",
    "    pool_strides = (2, 2)\n",
    "    cnn = conv2d_maxpool(x, conv_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    flat = tf.nn.dropout(flatten(cnn), keep_prob)    \n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    full_1 = tf.nn.dropout(fully_conn(flat, conv_outputs), keep_prob)  \n",
    "    full_2 = tf.nn.dropout(fully_conn(full_1, conv_outputs), keep_prob)  \n",
    "\n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    out = output(full_2, num_outputs)\n",
    "    \n",
    "    # TODO: return output\n",
    "    return out\n",
    " \n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    session.run(optimizer, feed_dict={x: feature_batch, y: label_batch, keep_prob: keep_probability})\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    loss = session.run(cost, feed_dict={x: feature_batch, y: label_batch, keep_prob: 1.0})   \n",
    "    acc = session.run(accuracy, feed_dict={x: valid_features, y: valid_labels, keep_prob: 1.0})        \n",
    "    print('validation accuracy = {:.6f}       loss = {:.6f}'.format(acc, loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 100\n",
    "batch_size = 1024\n",
    "keep_probability = 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  validation accuracy = 0.097800       loss = 2.302583\n",
      "Epoch  2, CIFAR-10 Batch 1:  validation accuracy = 0.097800       loss = 2.302583\n",
      "Epoch  3, CIFAR-10 Batch 1:  validation accuracy = 0.098200       loss = 2.302583\n",
      "Epoch  4, CIFAR-10 Batch 1:  validation accuracy = 0.152800       loss = 2.297883\n",
      "Epoch  5, CIFAR-10 Batch 1:  validation accuracy = 0.179200       loss = 2.259669\n",
      "Epoch  6, CIFAR-10 Batch 1:  validation accuracy = 0.220000       loss = 2.206279\n",
      "Epoch  7, CIFAR-10 Batch 1:  validation accuracy = 0.268200       loss = 2.123591\n",
      "Epoch  8, CIFAR-10 Batch 1:  validation accuracy = 0.288000       loss = 2.039723\n",
      "Epoch  9, CIFAR-10 Batch 1:  validation accuracy = 0.340800       loss = 1.987806\n",
      "Epoch 10, CIFAR-10 Batch 1:  validation accuracy = 0.324400       loss = 1.934737\n",
      "Epoch 11, CIFAR-10 Batch 1:  validation accuracy = 0.362800       loss = 1.882485\n",
      "Epoch 12, CIFAR-10 Batch 1:  validation accuracy = 0.373800       loss = 1.818282\n",
      "Epoch 13, CIFAR-10 Batch 1:  validation accuracy = 0.389000       loss = 1.777286\n",
      "Epoch 14, CIFAR-10 Batch 1:  validation accuracy = 0.406400       loss = 1.744603\n",
      "Epoch 15, CIFAR-10 Batch 1:  validation accuracy = 0.405800       loss = 1.711648\n",
      "Epoch 16, CIFAR-10 Batch 1:  validation accuracy = 0.412200       loss = 1.678692\n",
      "Epoch 17, CIFAR-10 Batch 1:  validation accuracy = 0.433400       loss = 1.612479\n",
      "Epoch 18, CIFAR-10 Batch 1:  validation accuracy = 0.433800       loss = 1.580246\n",
      "Epoch 19, CIFAR-10 Batch 1:  validation accuracy = 0.440600       loss = 1.554932\n",
      "Epoch 20, CIFAR-10 Batch 1:  validation accuracy = 0.447400       loss = 1.526660\n",
      "Epoch 21, CIFAR-10 Batch 1:  validation accuracy = 0.446800       loss = 1.495851\n",
      "Epoch 22, CIFAR-10 Batch 1:  validation accuracy = 0.454800       loss = 1.491462\n",
      "Epoch 23, CIFAR-10 Batch 1:  validation accuracy = 0.460600       loss = 1.463500\n",
      "Epoch 24, CIFAR-10 Batch 1:  validation accuracy = 0.471600       loss = 1.453798\n",
      "Epoch 25, CIFAR-10 Batch 1:  validation accuracy = 0.458600       loss = 1.430324\n",
      "Epoch 26, CIFAR-10 Batch 1:  validation accuracy = 0.473000       loss = 1.405828\n",
      "Epoch 27, CIFAR-10 Batch 1:  validation accuracy = 0.475000       loss = 1.388256\n",
      "Epoch 28, CIFAR-10 Batch 1:  validation accuracy = 0.471000       loss = 1.385010\n",
      "Epoch 29, CIFAR-10 Batch 1:  validation accuracy = 0.478000       loss = 1.362084\n",
      "Epoch 30, CIFAR-10 Batch 1:  validation accuracy = 0.483400       loss = 1.340831\n",
      "Epoch 31, CIFAR-10 Batch 1:  validation accuracy = 0.490800       loss = 1.324865\n",
      "Epoch 32, CIFAR-10 Batch 1:  validation accuracy = 0.490000       loss = 1.321414\n",
      "Epoch 33, CIFAR-10 Batch 1:  validation accuracy = 0.493400       loss = 1.307178\n",
      "Epoch 34, CIFAR-10 Batch 1:  validation accuracy = 0.489200       loss = 1.296967\n",
      "Epoch 35, CIFAR-10 Batch 1:  validation accuracy = 0.494600       loss = 1.280320\n",
      "Epoch 36, CIFAR-10 Batch 1:  validation accuracy = 0.498800       loss = 1.260272\n",
      "Epoch 37, CIFAR-10 Batch 1:  validation accuracy = 0.498600       loss = 1.247184\n",
      "Epoch 38, CIFAR-10 Batch 1:  validation accuracy = 0.512000       loss = 1.236821\n",
      "Epoch 39, CIFAR-10 Batch 1:  validation accuracy = 0.514600       loss = 1.223831\n",
      "Epoch 40, CIFAR-10 Batch 1:  validation accuracy = 0.512600       loss = 1.224570\n",
      "Epoch 41, CIFAR-10 Batch 1:  validation accuracy = 0.509200       loss = 1.204575\n",
      "Epoch 42, CIFAR-10 Batch 1:  validation accuracy = 0.516600       loss = 1.183409\n",
      "Epoch 43, CIFAR-10 Batch 1:  validation accuracy = 0.519400       loss = 1.179706\n",
      "Epoch 44, CIFAR-10 Batch 1:  validation accuracy = 0.515200       loss = 1.177328\n",
      "Epoch 45, CIFAR-10 Batch 1:  validation accuracy = 0.519400       loss = 1.150388\n",
      "Epoch 46, CIFAR-10 Batch 1:  validation accuracy = 0.523400       loss = 1.141184\n",
      "Epoch 47, CIFAR-10 Batch 1:  validation accuracy = 0.521800       loss = 1.127461\n",
      "Epoch 48, CIFAR-10 Batch 1:  validation accuracy = 0.529200       loss = 1.124886\n",
      "Epoch 49, CIFAR-10 Batch 1:  validation accuracy = 0.525800       loss = 1.115193\n",
      "Epoch 50, CIFAR-10 Batch 1:  validation accuracy = 0.527200       loss = 1.106719\n",
      "Epoch 51, CIFAR-10 Batch 1:  validation accuracy = 0.532600       loss = 1.102132\n",
      "Epoch 52, CIFAR-10 Batch 1:  validation accuracy = 0.528800       loss = 1.092552\n",
      "Epoch 53, CIFAR-10 Batch 1:  validation accuracy = 0.537000       loss = 1.078370\n",
      "Epoch 54, CIFAR-10 Batch 1:  validation accuracy = 0.531000       loss = 1.073914\n",
      "Epoch 55, CIFAR-10 Batch 1:  validation accuracy = 0.530000       loss = 1.053585\n",
      "Epoch 56, CIFAR-10 Batch 1:  validation accuracy = 0.533600       loss = 1.066564\n",
      "Epoch 57, CIFAR-10 Batch 1:  validation accuracy = 0.530200       loss = 1.043486\n",
      "Epoch 58, CIFAR-10 Batch 1:  validation accuracy = 0.529400       loss = 1.038065\n",
      "Epoch 59, CIFAR-10 Batch 1:  validation accuracy = 0.538000       loss = 1.018852\n",
      "Epoch 60, CIFAR-10 Batch 1:  validation accuracy = 0.540200       loss = 1.011786\n",
      "Epoch 61, CIFAR-10 Batch 1:  validation accuracy = 0.538000       loss = 0.998164\n",
      "Epoch 62, CIFAR-10 Batch 1:  validation accuracy = 0.538200       loss = 1.016990\n",
      "Epoch 63, CIFAR-10 Batch 1:  validation accuracy = 0.536400       loss = 0.997286\n",
      "Epoch 64, CIFAR-10 Batch 1:  validation accuracy = 0.539000       loss = 0.992338\n",
      "Epoch 65, CIFAR-10 Batch 1:  validation accuracy = 0.540200       loss = 0.976645\n",
      "Epoch 66, CIFAR-10 Batch 1:  validation accuracy = 0.540600       loss = 0.977521\n",
      "Epoch 67, CIFAR-10 Batch 1:  validation accuracy = 0.543200       loss = 0.965069\n",
      "Epoch 68, CIFAR-10 Batch 1:  validation accuracy = 0.543600       loss = 0.968829\n",
      "Epoch 69, CIFAR-10 Batch 1:  validation accuracy = 0.537000       loss = 0.963386\n",
      "Epoch 70, CIFAR-10 Batch 1:  validation accuracy = 0.545600       loss = 0.941611\n",
      "Epoch 71, CIFAR-10 Batch 1:  validation accuracy = 0.535800       loss = 0.933461\n",
      "Epoch 72, CIFAR-10 Batch 1:  validation accuracy = 0.540600       loss = 0.927580\n",
      "Epoch 73, CIFAR-10 Batch 1:  validation accuracy = 0.538800       loss = 0.930651\n",
      "Epoch 74, CIFAR-10 Batch 1:  validation accuracy = 0.544600       loss = 0.929003\n",
      "Epoch 75, CIFAR-10 Batch 1:  validation accuracy = 0.542800       loss = 0.930951\n",
      "Epoch 76, CIFAR-10 Batch 1:  validation accuracy = 0.544800       loss = 0.912715\n",
      "Epoch 77, CIFAR-10 Batch 1:  validation accuracy = 0.547800       loss = 0.890221\n",
      "Epoch 78, CIFAR-10 Batch 1:  validation accuracy = 0.547600       loss = 0.897319\n",
      "Epoch 79, CIFAR-10 Batch 1:  validation accuracy = 0.541600       loss = 0.899432\n",
      "Epoch 80, CIFAR-10 Batch 1:  validation accuracy = 0.550000       loss = 0.888488\n",
      "Epoch 81, CIFAR-10 Batch 1:  validation accuracy = 0.547000       loss = 0.877615\n",
      "Epoch 82, CIFAR-10 Batch 1:  validation accuracy = 0.548400       loss = 0.881263\n",
      "Epoch 83, CIFAR-10 Batch 1:  validation accuracy = 0.558000       loss = 0.858766\n",
      "Epoch 84, CIFAR-10 Batch 1:  validation accuracy = 0.551600       loss = 0.858223\n",
      "Epoch 85, CIFAR-10 Batch 1:  validation accuracy = 0.548200       loss = 0.851654\n",
      "Epoch 86, CIFAR-10 Batch 1:  validation accuracy = 0.548000       loss = 0.864097\n",
      "Epoch 87, CIFAR-10 Batch 1:  validation accuracy = 0.547400       loss = 0.846260\n",
      "Epoch 88, CIFAR-10 Batch 1:  validation accuracy = 0.550600       loss = 0.826964\n",
      "Epoch 89, CIFAR-10 Batch 1:  validation accuracy = 0.550200       loss = 0.834354\n",
      "Epoch 90, CIFAR-10 Batch 1:  validation accuracy = 0.551800       loss = 0.824653\n",
      "Epoch 91, CIFAR-10 Batch 1:  validation accuracy = 0.558400       loss = 0.808996\n",
      "Epoch 92, CIFAR-10 Batch 1:  validation accuracy = 0.551600       loss = 0.811727\n",
      "Epoch 93, CIFAR-10 Batch 1:  validation accuracy = 0.550800       loss = 0.819713\n",
      "Epoch 94, CIFAR-10 Batch 1:  validation accuracy = 0.552600       loss = 0.800058\n",
      "Epoch 95, CIFAR-10 Batch 1:  validation accuracy = 0.551600       loss = 0.797263\n",
      "Epoch 96, CIFAR-10 Batch 1:  validation accuracy = 0.550800       loss = 0.791120\n",
      "Epoch 97, CIFAR-10 Batch 1:  validation accuracy = 0.552600       loss = 0.788161\n",
      "Epoch 98, CIFAR-10 Batch 1:  validation accuracy = 0.558200       loss = 0.774766\n",
      "Epoch 99, CIFAR-10 Batch 1:  validation accuracy = 0.552600       loss = 0.764117\n",
      "Epoch 100, CIFAR-10 Batch 1:  validation accuracy = 0.551600       loss = 0.769271\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  validation accuracy = 0.110800       loss = 2.299854\n",
      "Epoch  1, CIFAR-10 Batch 2:  validation accuracy = 0.179400       loss = 2.296597\n",
      "Epoch  1, CIFAR-10 Batch 3:  validation accuracy = 0.214600       loss = 2.283062\n",
      "Epoch  1, CIFAR-10 Batch 4:  validation accuracy = 0.184800       loss = 2.247756\n",
      "Epoch  1, CIFAR-10 Batch 5:  validation accuracy = 0.211400       loss = 2.208574\n",
      "Epoch  2, CIFAR-10 Batch 1:  validation accuracy = 0.241800       loss = 2.161970\n",
      "Epoch  2, CIFAR-10 Batch 2:  validation accuracy = 0.262200       loss = 2.118465\n",
      "Epoch  2, CIFAR-10 Batch 3:  validation accuracy = 0.274800       loss = 2.087342\n",
      "Epoch  2, CIFAR-10 Batch 4:  validation accuracy = 0.287000       loss = 1.994031\n",
      "Epoch  2, CIFAR-10 Batch 5:  validation accuracy = 0.303000       loss = 2.003503\n",
      "Epoch  3, CIFAR-10 Batch 1:  validation accuracy = 0.326600       loss = 1.925171\n",
      "Epoch  3, CIFAR-10 Batch 2:  validation accuracy = 0.336200       loss = 1.919614\n",
      "Epoch  3, CIFAR-10 Batch 3:  validation accuracy = 0.354400       loss = 1.839968\n",
      "Epoch  3, CIFAR-10 Batch 4:  validation accuracy = 0.376800       loss = 1.794845\n",
      "Epoch  3, CIFAR-10 Batch 5:  validation accuracy = 0.370200       loss = 1.797669\n",
      "Epoch  4, CIFAR-10 Batch 1:  validation accuracy = 0.386600       loss = 1.758101\n",
      "Epoch  4, CIFAR-10 Batch 2:  validation accuracy = 0.404800       loss = 1.745297\n",
      "Epoch  4, CIFAR-10 Batch 3:  validation accuracy = 0.393800       loss = 1.683309\n",
      "Epoch  4, CIFAR-10 Batch 4:  validation accuracy = 0.415000       loss = 1.663214\n",
      "Epoch  4, CIFAR-10 Batch 5:  validation accuracy = 0.416000       loss = 1.685471\n",
      "Epoch  5, CIFAR-10 Batch 1:  validation accuracy = 0.427000       loss = 1.637313\n",
      "Epoch  5, CIFAR-10 Batch 2:  validation accuracy = 0.441600       loss = 1.654076\n",
      "Epoch  5, CIFAR-10 Batch 3:  validation accuracy = 0.440800       loss = 1.574443\n",
      "Epoch  5, CIFAR-10 Batch 4:  validation accuracy = 0.454200       loss = 1.563659\n",
      "Epoch  5, CIFAR-10 Batch 5:  validation accuracy = 0.444400       loss = 1.617428\n",
      "Epoch  6, CIFAR-10 Batch 1:  validation accuracy = 0.463400       loss = 1.534377\n",
      "Epoch  6, CIFAR-10 Batch 2:  validation accuracy = 0.451400       loss = 1.602718\n",
      "Epoch  6, CIFAR-10 Batch 3:  validation accuracy = 0.456400       loss = 1.544309\n",
      "Epoch  6, CIFAR-10 Batch 4:  validation accuracy = 0.469800       loss = 1.503913\n",
      "Epoch  6, CIFAR-10 Batch 5:  validation accuracy = 0.464200       loss = 1.536787\n",
      "Epoch  7, CIFAR-10 Batch 1:  validation accuracy = 0.476600       loss = 1.501869\n",
      "Epoch  7, CIFAR-10 Batch 2:  validation accuracy = 0.476000       loss = 1.527482\n",
      "Epoch  7, CIFAR-10 Batch 3:  validation accuracy = 0.480000       loss = 1.470470\n",
      "Epoch  7, CIFAR-10 Batch 4:  validation accuracy = 0.481200       loss = 1.448963\n",
      "Epoch  7, CIFAR-10 Batch 5:  validation accuracy = 0.489400       loss = 1.469933\n",
      "Epoch  8, CIFAR-10 Batch 1:  validation accuracy = 0.492600       loss = 1.447062\n",
      "Epoch  8, CIFAR-10 Batch 2:  validation accuracy = 0.492400       loss = 1.487655\n",
      "Epoch  8, CIFAR-10 Batch 3:  validation accuracy = 0.493000       loss = 1.408694\n",
      "Epoch  8, CIFAR-10 Batch 4:  validation accuracy = 0.496400       loss = 1.415473\n",
      "Epoch  8, CIFAR-10 Batch 5:  validation accuracy = 0.485200       loss = 1.437263\n",
      "Epoch  9, CIFAR-10 Batch 1:  validation accuracy = 0.502200       loss = 1.408716\n",
      "Epoch  9, CIFAR-10 Batch 2:  validation accuracy = 0.503800       loss = 1.452030\n",
      "Epoch  9, CIFAR-10 Batch 3:  validation accuracy = 0.503200       loss = 1.361055\n",
      "Epoch  9, CIFAR-10 Batch 4:  validation accuracy = 0.510000       loss = 1.378042\n",
      "Epoch  9, CIFAR-10 Batch 5:  validation accuracy = 0.507000       loss = 1.398740\n",
      "Epoch 10, CIFAR-10 Batch 1:  validation accuracy = 0.515200       loss = 1.375134\n",
      "Epoch 10, CIFAR-10 Batch 2:  validation accuracy = 0.511800       loss = 1.424113\n",
      "Epoch 10, CIFAR-10 Batch 3:  validation accuracy = 0.508000       loss = 1.339172\n",
      "Epoch 10, CIFAR-10 Batch 4:  validation accuracy = 0.511800       loss = 1.380549\n",
      "Epoch 10, CIFAR-10 Batch 5:  validation accuracy = 0.514800       loss = 1.352121\n",
      "Epoch 11, CIFAR-10 Batch 1:  validation accuracy = 0.516000       loss = 1.367995\n",
      "Epoch 11, CIFAR-10 Batch 2:  validation accuracy = 0.518800       loss = 1.384790\n",
      "Epoch 11, CIFAR-10 Batch 3:  validation accuracy = 0.511400       loss = 1.321589\n",
      "Epoch 11, CIFAR-10 Batch 4:  validation accuracy = 0.517400       loss = 1.347623\n",
      "Epoch 11, CIFAR-10 Batch 5:  validation accuracy = 0.515000       loss = 1.332988\n",
      "Epoch 12, CIFAR-10 Batch 1:  validation accuracy = 0.525400       loss = 1.344982\n",
      "Epoch 12, CIFAR-10 Batch 2:  validation accuracy = 0.533400       loss = 1.365539\n",
      "Epoch 12, CIFAR-10 Batch 3:  validation accuracy = 0.528000       loss = 1.292622\n",
      "Epoch 12, CIFAR-10 Batch 4:  validation accuracy = 0.526400       loss = 1.317844\n",
      "Epoch 12, CIFAR-10 Batch 5:  validation accuracy = 0.528200       loss = 1.311682\n",
      "Epoch 13, CIFAR-10 Batch 1:  validation accuracy = 0.524400       loss = 1.320281\n",
      "Epoch 13, CIFAR-10 Batch 2:  validation accuracy = 0.534600       loss = 1.332564\n",
      "Epoch 13, CIFAR-10 Batch 3:  validation accuracy = 0.527200       loss = 1.285829\n",
      "Epoch 13, CIFAR-10 Batch 4:  validation accuracy = 0.535600       loss = 1.307714\n",
      "Epoch 13, CIFAR-10 Batch 5:  validation accuracy = 0.536000       loss = 1.294179\n",
      "Epoch 14, CIFAR-10 Batch 1:  validation accuracy = 0.534400       loss = 1.306352\n",
      "Epoch 14, CIFAR-10 Batch 2:  validation accuracy = 0.540000       loss = 1.324211\n",
      "Epoch 14, CIFAR-10 Batch 3:  validation accuracy = 0.537600       loss = 1.263372\n",
      "Epoch 14, CIFAR-10 Batch 4:  validation accuracy = 0.545200       loss = 1.283176\n",
      "Epoch 14, CIFAR-10 Batch 5:  validation accuracy = 0.546600       loss = 1.264287\n",
      "Epoch 15, CIFAR-10 Batch 1:  validation accuracy = 0.537200       loss = 1.288854\n",
      "Epoch 15, CIFAR-10 Batch 2:  validation accuracy = 0.544600       loss = 1.299274\n",
      "Epoch 15, CIFAR-10 Batch 3:  validation accuracy = 0.545800       loss = 1.225873\n",
      "Epoch 15, CIFAR-10 Batch 4:  validation accuracy = 0.546000       loss = 1.280228\n",
      "Epoch 15, CIFAR-10 Batch 5:  validation accuracy = 0.551000       loss = 1.261930\n",
      "Epoch 16, CIFAR-10 Batch 1:  validation accuracy = 0.552200       loss = 1.263059\n",
      "Epoch 16, CIFAR-10 Batch 2:  validation accuracy = 0.547800       loss = 1.293651\n",
      "Epoch 16, CIFAR-10 Batch 3:  validation accuracy = 0.548400       loss = 1.212050\n",
      "Epoch 16, CIFAR-10 Batch 4:  validation accuracy = 0.554400       loss = 1.247603\n",
      "Epoch 16, CIFAR-10 Batch 5:  validation accuracy = 0.552000       loss = 1.240685\n",
      "Epoch 17, CIFAR-10 Batch 1:  validation accuracy = 0.555600       loss = 1.240598\n",
      "Epoch 17, CIFAR-10 Batch 2:  validation accuracy = 0.545800       loss = 1.272596\n",
      "Epoch 17, CIFAR-10 Batch 3:  validation accuracy = 0.544600       loss = 1.209811\n",
      "Epoch 17, CIFAR-10 Batch 4:  validation accuracy = 0.545600       loss = 1.239544\n",
      "Epoch 17, CIFAR-10 Batch 5:  validation accuracy = 0.542200       loss = 1.219543\n",
      "Epoch 18, CIFAR-10 Batch 1:  validation accuracy = 0.551800       loss = 1.235317\n",
      "Epoch 18, CIFAR-10 Batch 2:  validation accuracy = 0.551800       loss = 1.250762\n",
      "Epoch 18, CIFAR-10 Batch 3:  validation accuracy = 0.549000       loss = 1.208941\n",
      "Epoch 18, CIFAR-10 Batch 4:  validation accuracy = 0.558600       loss = 1.225574\n",
      "Epoch 18, CIFAR-10 Batch 5:  validation accuracy = 0.553200       loss = 1.210614\n",
      "Epoch 19, CIFAR-10 Batch 1:  validation accuracy = 0.559600       loss = 1.225868\n",
      "Epoch 19, CIFAR-10 Batch 2:  validation accuracy = 0.558000       loss = 1.241145\n",
      "Epoch 19, CIFAR-10 Batch 3:  validation accuracy = 0.555200       loss = 1.193089\n",
      "Epoch 19, CIFAR-10 Batch 4:  validation accuracy = 0.557000       loss = 1.231058\n",
      "Epoch 19, CIFAR-10 Batch 5:  validation accuracy = 0.558400       loss = 1.201742\n",
      "Epoch 20, CIFAR-10 Batch 1:  validation accuracy = 0.564600       loss = 1.216099\n",
      "Epoch 20, CIFAR-10 Batch 2:  validation accuracy = 0.557400       loss = 1.218469\n",
      "Epoch 20, CIFAR-10 Batch 3:  validation accuracy = 0.559600       loss = 1.180390\n",
      "Epoch 20, CIFAR-10 Batch 4:  validation accuracy = 0.554400       loss = 1.208407\n",
      "Epoch 20, CIFAR-10 Batch 5:  validation accuracy = 0.567800       loss = 1.184788\n",
      "Epoch 21, CIFAR-10 Batch 1:  validation accuracy = 0.566200       loss = 1.205499\n",
      "Epoch 21, CIFAR-10 Batch 2:  validation accuracy = 0.562200       loss = 1.210878\n",
      "Epoch 21, CIFAR-10 Batch 3:  validation accuracy = 0.565200       loss = 1.171020\n",
      "Epoch 21, CIFAR-10 Batch 4:  validation accuracy = 0.569000       loss = 1.207321\n",
      "Epoch 21, CIFAR-10 Batch 5:  validation accuracy = 0.563600       loss = 1.168641\n",
      "Epoch 22, CIFAR-10 Batch 1:  validation accuracy = 0.559400       loss = 1.190041\n",
      "Epoch 22, CIFAR-10 Batch 2:  validation accuracy = 0.561800       loss = 1.194292\n",
      "Epoch 22, CIFAR-10 Batch 3:  validation accuracy = 0.563200       loss = 1.176831\n",
      "Epoch 22, CIFAR-10 Batch 4:  validation accuracy = 0.566800       loss = 1.176145\n",
      "Epoch 22, CIFAR-10 Batch 5:  validation accuracy = 0.566200       loss = 1.162385\n",
      "Epoch 23, CIFAR-10 Batch 1:  validation accuracy = 0.562200       loss = 1.190201\n",
      "Epoch 23, CIFAR-10 Batch 2:  validation accuracy = 0.565000       loss = 1.191354\n",
      "Epoch 23, CIFAR-10 Batch 3:  validation accuracy = 0.564200       loss = 1.167564\n",
      "Epoch 23, CIFAR-10 Batch 4:  validation accuracy = 0.567000       loss = 1.173403\n",
      "Epoch 23, CIFAR-10 Batch 5:  validation accuracy = 0.568800       loss = 1.150242\n",
      "Epoch 24, CIFAR-10 Batch 1:  validation accuracy = 0.571600       loss = 1.174555\n",
      "Epoch 24, CIFAR-10 Batch 2:  validation accuracy = 0.561600       loss = 1.200052\n",
      "Epoch 24, CIFAR-10 Batch 3:  validation accuracy = 0.562200       loss = 1.145526\n",
      "Epoch 24, CIFAR-10 Batch 4:  validation accuracy = 0.567000       loss = 1.172621\n",
      "Epoch 24, CIFAR-10 Batch 5:  validation accuracy = 0.560800       loss = 1.149379\n",
      "Epoch 25, CIFAR-10 Batch 1:  validation accuracy = 0.567600       loss = 1.169673\n",
      "Epoch 25, CIFAR-10 Batch 2:  validation accuracy = 0.569000       loss = 1.182651\n",
      "Epoch 25, CIFAR-10 Batch 3:  validation accuracy = 0.573000       loss = 1.137353\n",
      "Epoch 25, CIFAR-10 Batch 4:  validation accuracy = 0.572600       loss = 1.145653\n",
      "Epoch 25, CIFAR-10 Batch 5:  validation accuracy = 0.568800       loss = 1.135873\n",
      "Epoch 26, CIFAR-10 Batch 1:  validation accuracy = 0.572200       loss = 1.151567\n",
      "Epoch 26, CIFAR-10 Batch 2:  validation accuracy = 0.574600       loss = 1.166882\n",
      "Epoch 26, CIFAR-10 Batch 3:  validation accuracy = 0.580000       loss = 1.109882\n",
      "Epoch 26, CIFAR-10 Batch 4:  validation accuracy = 0.580400       loss = 1.152512\n",
      "Epoch 26, CIFAR-10 Batch 5:  validation accuracy = 0.577400       loss = 1.131414\n",
      "Epoch 27, CIFAR-10 Batch 1:  validation accuracy = 0.576800       loss = 1.149667\n",
      "Epoch 27, CIFAR-10 Batch 2:  validation accuracy = 0.569800       loss = 1.171632\n",
      "Epoch 27, CIFAR-10 Batch 3:  validation accuracy = 0.566600       loss = 1.142180\n",
      "Epoch 27, CIFAR-10 Batch 4:  validation accuracy = 0.575800       loss = 1.140362\n",
      "Epoch 27, CIFAR-10 Batch 5:  validation accuracy = 0.578800       loss = 1.124885\n",
      "Epoch 28, CIFAR-10 Batch 1:  validation accuracy = 0.574000       loss = 1.136977\n",
      "Epoch 28, CIFAR-10 Batch 2:  validation accuracy = 0.576800       loss = 1.148268\n",
      "Epoch 28, CIFAR-10 Batch 3:  validation accuracy = 0.577200       loss = 1.114368\n",
      "Epoch 28, CIFAR-10 Batch 4:  validation accuracy = 0.580800       loss = 1.136620\n",
      "Epoch 28, CIFAR-10 Batch 5:  validation accuracy = 0.582800       loss = 1.115146\n",
      "Epoch 29, CIFAR-10 Batch 1:  validation accuracy = 0.579200       loss = 1.132827\n",
      "Epoch 29, CIFAR-10 Batch 2:  validation accuracy = 0.576000       loss = 1.156879\n",
      "Epoch 29, CIFAR-10 Batch 3:  validation accuracy = 0.580600       loss = 1.102733\n",
      "Epoch 29, CIFAR-10 Batch 4:  validation accuracy = 0.578400       loss = 1.116084\n",
      "Epoch 29, CIFAR-10 Batch 5:  validation accuracy = 0.583800       loss = 1.109730\n",
      "Epoch 30, CIFAR-10 Batch 1:  validation accuracy = 0.580600       loss = 1.124554\n",
      "Epoch 30, CIFAR-10 Batch 2:  validation accuracy = 0.583200       loss = 1.130094\n",
      "Epoch 30, CIFAR-10 Batch 3:  validation accuracy = 0.572400       loss = 1.123399\n",
      "Epoch 30, CIFAR-10 Batch 4:  validation accuracy = 0.584200       loss = 1.103392\n",
      "Epoch 30, CIFAR-10 Batch 5:  validation accuracy = 0.584200       loss = 1.101839\n",
      "Epoch 31, CIFAR-10 Batch 1:  validation accuracy = 0.583000       loss = 1.116250\n",
      "Epoch 31, CIFAR-10 Batch 2:  validation accuracy = 0.580800       loss = 1.130625\n",
      "Epoch 31, CIFAR-10 Batch 3:  validation accuracy = 0.580400       loss = 1.102687\n",
      "Epoch 31, CIFAR-10 Batch 4:  validation accuracy = 0.583400       loss = 1.114922\n",
      "Epoch 31, CIFAR-10 Batch 5:  validation accuracy = 0.588400       loss = 1.088960\n",
      "Epoch 32, CIFAR-10 Batch 1:  validation accuracy = 0.589000       loss = 1.104064\n",
      "Epoch 32, CIFAR-10 Batch 2:  validation accuracy = 0.584400       loss = 1.109613\n",
      "Epoch 32, CIFAR-10 Batch 3:  validation accuracy = 0.586400       loss = 1.096813\n",
      "Epoch 32, CIFAR-10 Batch 4:  validation accuracy = 0.573400       loss = 1.104076\n",
      "Epoch 32, CIFAR-10 Batch 5:  validation accuracy = 0.589000       loss = 1.091395\n",
      "Epoch 33, CIFAR-10 Batch 1:  validation accuracy = 0.585600       loss = 1.092694\n",
      "Epoch 33, CIFAR-10 Batch 2:  validation accuracy = 0.579800       loss = 1.121714\n",
      "Epoch 33, CIFAR-10 Batch 3:  validation accuracy = 0.584800       loss = 1.087739\n",
      "Epoch 33, CIFAR-10 Batch 4:  validation accuracy = 0.590800       loss = 1.102624\n",
      "Epoch 33, CIFAR-10 Batch 5:  validation accuracy = 0.588800       loss = 1.075021\n",
      "Epoch 34, CIFAR-10 Batch 1:  validation accuracy = 0.585000       loss = 1.097218\n",
      "Epoch 34, CIFAR-10 Batch 2:  validation accuracy = 0.584400       loss = 1.118269\n",
      "Epoch 34, CIFAR-10 Batch 3:  validation accuracy = 0.593000       loss = 1.082541\n",
      "Epoch 34, CIFAR-10 Batch 4:  validation accuracy = 0.582400       loss = 1.098657\n",
      "Epoch 34, CIFAR-10 Batch 5:  validation accuracy = 0.588200       loss = 1.089054\n",
      "Epoch 35, CIFAR-10 Batch 1:  validation accuracy = 0.586600       loss = 1.089770\n",
      "Epoch 35, CIFAR-10 Batch 2:  validation accuracy = 0.581800       loss = 1.107510\n",
      "Epoch 35, CIFAR-10 Batch 3:  validation accuracy = 0.588600       loss = 1.071500\n",
      "Epoch 35, CIFAR-10 Batch 4:  validation accuracy = 0.593400       loss = 1.083605\n",
      "Epoch 35, CIFAR-10 Batch 5:  validation accuracy = 0.593400       loss = 1.070647\n",
      "Epoch 36, CIFAR-10 Batch 1:  validation accuracy = 0.589200       loss = 1.090834\n",
      "Epoch 36, CIFAR-10 Batch 2:  validation accuracy = 0.588600       loss = 1.093234\n",
      "Epoch 36, CIFAR-10 Batch 3:  validation accuracy = 0.588400       loss = 1.067032\n",
      "Epoch 36, CIFAR-10 Batch 4:  validation accuracy = 0.591600       loss = 1.067146\n",
      "Epoch 36, CIFAR-10 Batch 5:  validation accuracy = 0.592000       loss = 1.056201\n",
      "Epoch 37, CIFAR-10 Batch 1:  validation accuracy = 0.593400       loss = 1.079005\n",
      "Epoch 37, CIFAR-10 Batch 2:  validation accuracy = 0.591000       loss = 1.084686\n",
      "Epoch 37, CIFAR-10 Batch 3:  validation accuracy = 0.597600       loss = 1.059484\n",
      "Epoch 37, CIFAR-10 Batch 4:  validation accuracy = 0.586400       loss = 1.079283\n",
      "Epoch 37, CIFAR-10 Batch 5:  validation accuracy = 0.584200       loss = 1.071895\n",
      "Epoch 38, CIFAR-10 Batch 1:  validation accuracy = 0.588000       loss = 1.075216\n",
      "Epoch 38, CIFAR-10 Batch 2:  validation accuracy = 0.586400       loss = 1.093140\n",
      "Epoch 38, CIFAR-10 Batch 3:  validation accuracy = 0.600000       loss = 1.061383\n",
      "Epoch 38, CIFAR-10 Batch 4:  validation accuracy = 0.596800       loss = 1.065318\n",
      "Epoch 38, CIFAR-10 Batch 5:  validation accuracy = 0.596400       loss = 1.055187\n",
      "Epoch 39, CIFAR-10 Batch 1:  validation accuracy = 0.589800       loss = 1.076860\n",
      "Epoch 39, CIFAR-10 Batch 2:  validation accuracy = 0.590000       loss = 1.079491\n",
      "Epoch 39, CIFAR-10 Batch 3:  validation accuracy = 0.599200       loss = 1.039249\n",
      "Epoch 39, CIFAR-10 Batch 4:  validation accuracy = 0.600800       loss = 1.049661\n",
      "Epoch 39, CIFAR-10 Batch 5:  validation accuracy = 0.597000       loss = 1.055251\n",
      "Epoch 40, CIFAR-10 Batch 1:  validation accuracy = 0.598200       loss = 1.078796\n",
      "Epoch 40, CIFAR-10 Batch 2:  validation accuracy = 0.586800       loss = 1.085012\n",
      "Epoch 40, CIFAR-10 Batch 3:  validation accuracy = 0.595600       loss = 1.071885\n",
      "Epoch 40, CIFAR-10 Batch 4:  validation accuracy = 0.595600       loss = 1.047048\n",
      "Epoch 40, CIFAR-10 Batch 5:  validation accuracy = 0.600000       loss = 1.039306\n",
      "Epoch 41, CIFAR-10 Batch 1:  validation accuracy = 0.594600       loss = 1.070691\n",
      "Epoch 41, CIFAR-10 Batch 2:  validation accuracy = 0.590400       loss = 1.082192\n",
      "Epoch 41, CIFAR-10 Batch 3:  validation accuracy = 0.596600       loss = 1.028948\n",
      "Epoch 41, CIFAR-10 Batch 4:  validation accuracy = 0.605200       loss = 1.046989\n",
      "Epoch 41, CIFAR-10 Batch 5:  validation accuracy = 0.599400       loss = 1.042331\n",
      "Epoch 42, CIFAR-10 Batch 1:  validation accuracy = 0.601200       loss = 1.050215\n",
      "Epoch 42, CIFAR-10 Batch 2:  validation accuracy = 0.597000       loss = 1.067642\n",
      "Epoch 42, CIFAR-10 Batch 3:  validation accuracy = 0.601600       loss = 1.036207\n",
      "Epoch 42, CIFAR-10 Batch 4:  validation accuracy = 0.599800       loss = 1.047436\n",
      "Epoch 42, CIFAR-10 Batch 5:  validation accuracy = 0.602400       loss = 1.024663\n",
      "Epoch 43, CIFAR-10 Batch 1:  validation accuracy = 0.604000       loss = 1.037901\n",
      "Epoch 43, CIFAR-10 Batch 2:  validation accuracy = 0.602000       loss = 1.050985\n",
      "Epoch 43, CIFAR-10 Batch 3:  validation accuracy = 0.600200       loss = 1.023847\n",
      "Epoch 43, CIFAR-10 Batch 4:  validation accuracy = 0.592600       loss = 1.041745\n",
      "Epoch 43, CIFAR-10 Batch 5:  validation accuracy = 0.603400       loss = 1.025606\n",
      "Epoch 44, CIFAR-10 Batch 1:  validation accuracy = 0.604800       loss = 1.045062\n",
      "Epoch 44, CIFAR-10 Batch 2:  validation accuracy = 0.602600       loss = 1.052000\n",
      "Epoch 44, CIFAR-10 Batch 3:  validation accuracy = 0.593800       loss = 1.032694\n",
      "Epoch 44, CIFAR-10 Batch 4:  validation accuracy = 0.600800       loss = 1.029547\n",
      "Epoch 44, CIFAR-10 Batch 5:  validation accuracy = 0.613400       loss = 1.018704\n",
      "Epoch 45, CIFAR-10 Batch 1:  validation accuracy = 0.611800       loss = 1.038306\n",
      "Epoch 45, CIFAR-10 Batch 2:  validation accuracy = 0.611800       loss = 1.050068\n",
      "Epoch 45, CIFAR-10 Batch 3:  validation accuracy = 0.606800       loss = 1.025618\n",
      "Epoch 45, CIFAR-10 Batch 4:  validation accuracy = 0.603200       loss = 1.010816\n",
      "Epoch 45, CIFAR-10 Batch 5:  validation accuracy = 0.612600       loss = 1.008664\n",
      "Epoch 46, CIFAR-10 Batch 1:  validation accuracy = 0.602800       loss = 1.034476\n",
      "Epoch 46, CIFAR-10 Batch 2:  validation accuracy = 0.610200       loss = 1.044075\n",
      "Epoch 46, CIFAR-10 Batch 3:  validation accuracy = 0.604200       loss = 1.021321\n",
      "Epoch 46, CIFAR-10 Batch 4:  validation accuracy = 0.607600       loss = 1.006099\n",
      "Epoch 46, CIFAR-10 Batch 5:  validation accuracy = 0.608400       loss = 1.012654\n",
      "Epoch 47, CIFAR-10 Batch 1:  validation accuracy = 0.597400       loss = 1.040859\n",
      "Epoch 47, CIFAR-10 Batch 2:  validation accuracy = 0.612200       loss = 1.044825\n",
      "Epoch 47, CIFAR-10 Batch 3:  validation accuracy = 0.613200       loss = 1.004098\n",
      "Epoch 47, CIFAR-10 Batch 4:  validation accuracy = 0.609600       loss = 1.006003\n",
      "Epoch 47, CIFAR-10 Batch 5:  validation accuracy = 0.611200       loss = 1.004481\n",
      "Epoch 48, CIFAR-10 Batch 1:  validation accuracy = 0.603400       loss = 1.028398\n",
      "Epoch 48, CIFAR-10 Batch 2:  validation accuracy = 0.609800       loss = 1.029259\n",
      "Epoch 48, CIFAR-10 Batch 3:  validation accuracy = 0.599000       loss = 1.005537\n",
      "Epoch 48, CIFAR-10 Batch 4:  validation accuracy = 0.606400       loss = 1.009320\n",
      "Epoch 48, CIFAR-10 Batch 5:  validation accuracy = 0.612400       loss = 0.994570\n",
      "Epoch 49, CIFAR-10 Batch 1:  validation accuracy = 0.612800       loss = 1.023102\n",
      "Epoch 49, CIFAR-10 Batch 2:  validation accuracy = 0.611800       loss = 1.024956\n",
      "Epoch 49, CIFAR-10 Batch 3:  validation accuracy = 0.607000       loss = 0.986408\n",
      "Epoch 49, CIFAR-10 Batch 4:  validation accuracy = 0.613400       loss = 1.004198\n",
      "Epoch 49, CIFAR-10 Batch 5:  validation accuracy = 0.612000       loss = 0.986153\n",
      "Epoch 50, CIFAR-10 Batch 1:  validation accuracy = 0.608600       loss = 1.010900\n",
      "Epoch 50, CIFAR-10 Batch 2:  validation accuracy = 0.612200       loss = 1.022439\n",
      "Epoch 50, CIFAR-10 Batch 3:  validation accuracy = 0.611400       loss = 0.987832\n",
      "Epoch 50, CIFAR-10 Batch 4:  validation accuracy = 0.612600       loss = 0.983929\n",
      "Epoch 50, CIFAR-10 Batch 5:  validation accuracy = 0.610000       loss = 0.988418\n",
      "Epoch 51, CIFAR-10 Batch 1:  validation accuracy = 0.609400       loss = 1.015898\n",
      "Epoch 51, CIFAR-10 Batch 2:  validation accuracy = 0.620400       loss = 1.003491\n",
      "Epoch 51, CIFAR-10 Batch 3:  validation accuracy = 0.614000       loss = 0.986534\n",
      "Epoch 51, CIFAR-10 Batch 4:  validation accuracy = 0.608000       loss = 0.989786\n",
      "Epoch 51, CIFAR-10 Batch 5:  validation accuracy = 0.607400       loss = 0.976456\n",
      "Epoch 52, CIFAR-10 Batch 1:  validation accuracy = 0.610200       loss = 1.003743\n",
      "Epoch 52, CIFAR-10 Batch 2:  validation accuracy = 0.615200       loss = 1.003626\n",
      "Epoch 52, CIFAR-10 Batch 3:  validation accuracy = 0.620200       loss = 0.972364\n",
      "Epoch 52, CIFAR-10 Batch 4:  validation accuracy = 0.607400       loss = 0.976196\n",
      "Epoch 52, CIFAR-10 Batch 5:  validation accuracy = 0.621400       loss = 0.964406\n",
      "Epoch 53, CIFAR-10 Batch 1:  validation accuracy = 0.610600       loss = 0.999785\n",
      "Epoch 53, CIFAR-10 Batch 2:  validation accuracy = 0.617200       loss = 1.000696\n",
      "Epoch 53, CIFAR-10 Batch 3:  validation accuracy = 0.611000       loss = 0.986698\n",
      "Epoch 53, CIFAR-10 Batch 4:  validation accuracy = 0.610800       loss = 0.975114\n",
      "Epoch 53, CIFAR-10 Batch 5:  validation accuracy = 0.616000       loss = 0.974468\n",
      "Epoch 54, CIFAR-10 Batch 1:  validation accuracy = 0.614600       loss = 0.998234\n",
      "Epoch 54, CIFAR-10 Batch 2:  validation accuracy = 0.611400       loss = 1.008899\n",
      "Epoch 54, CIFAR-10 Batch 3:  validation accuracy = 0.609400       loss = 0.976814\n",
      "Epoch 54, CIFAR-10 Batch 4:  validation accuracy = 0.612000       loss = 0.972726\n",
      "Epoch 54, CIFAR-10 Batch 5:  validation accuracy = 0.615000       loss = 0.963050\n",
      "Epoch 55, CIFAR-10 Batch 1:  validation accuracy = 0.611600       loss = 0.993303\n",
      "Epoch 55, CIFAR-10 Batch 2:  validation accuracy = 0.615400       loss = 0.995847\n",
      "Epoch 55, CIFAR-10 Batch 3:  validation accuracy = 0.610200       loss = 0.967573\n",
      "Epoch 55, CIFAR-10 Batch 4:  validation accuracy = 0.617400       loss = 0.966219\n",
      "Epoch 55, CIFAR-10 Batch 5:  validation accuracy = 0.618600       loss = 0.959901\n",
      "Epoch 56, CIFAR-10 Batch 1:  validation accuracy = 0.616200       loss = 0.998293\n",
      "Epoch 56, CIFAR-10 Batch 2:  validation accuracy = 0.615000       loss = 0.988897\n",
      "Epoch 56, CIFAR-10 Batch 3:  validation accuracy = 0.621800       loss = 0.956370\n",
      "Epoch 56, CIFAR-10 Batch 4:  validation accuracy = 0.609200       loss = 0.973638\n",
      "Epoch 56, CIFAR-10 Batch 5:  validation accuracy = 0.613800       loss = 0.950640\n",
      "Epoch 57, CIFAR-10 Batch 1:  validation accuracy = 0.622600       loss = 0.981464\n",
      "Epoch 57, CIFAR-10 Batch 2:  validation accuracy = 0.625000       loss = 0.971080\n",
      "Epoch 57, CIFAR-10 Batch 3:  validation accuracy = 0.619200       loss = 0.963406\n",
      "Epoch 57, CIFAR-10 Batch 4:  validation accuracy = 0.614000       loss = 0.964945\n",
      "Epoch 57, CIFAR-10 Batch 5:  validation accuracy = 0.612200       loss = 0.956454\n",
      "Epoch 58, CIFAR-10 Batch 1:  validation accuracy = 0.620400       loss = 0.974369\n",
      "Epoch 58, CIFAR-10 Batch 2:  validation accuracy = 0.619200       loss = 0.982233\n",
      "Epoch 58, CIFAR-10 Batch 3:  validation accuracy = 0.626800       loss = 0.950124\n",
      "Epoch 58, CIFAR-10 Batch 4:  validation accuracy = 0.615400       loss = 0.958323\n",
      "Epoch 58, CIFAR-10 Batch 5:  validation accuracy = 0.626800       loss = 0.932032\n",
      "Epoch 59, CIFAR-10 Batch 1:  validation accuracy = 0.617400       loss = 0.965773\n",
      "Epoch 59, CIFAR-10 Batch 2:  validation accuracy = 0.621600       loss = 0.968780\n",
      "Epoch 59, CIFAR-10 Batch 3:  validation accuracy = 0.624600       loss = 0.945350\n",
      "Epoch 59, CIFAR-10 Batch 4:  validation accuracy = 0.617000       loss = 0.955388\n",
      "Epoch 59, CIFAR-10 Batch 5:  validation accuracy = 0.621200       loss = 0.945657\n",
      "Epoch 60, CIFAR-10 Batch 1:  validation accuracy = 0.625200       loss = 0.952907\n",
      "Epoch 60, CIFAR-10 Batch 2:  validation accuracy = 0.614200       loss = 0.978972\n",
      "Epoch 60, CIFAR-10 Batch 3:  validation accuracy = 0.622400       loss = 0.947850\n",
      "Epoch 60, CIFAR-10 Batch 4:  validation accuracy = 0.625000       loss = 0.937167\n",
      "Epoch 60, CIFAR-10 Batch 5:  validation accuracy = 0.624600       loss = 0.925972\n",
      "Epoch 61, CIFAR-10 Batch 1:  validation accuracy = 0.619800       loss = 0.950566\n",
      "Epoch 61, CIFAR-10 Batch 2:  validation accuracy = 0.623200       loss = 0.986846\n",
      "Epoch 61, CIFAR-10 Batch 3:  validation accuracy = 0.625600       loss = 0.949493\n",
      "Epoch 61, CIFAR-10 Batch 4:  validation accuracy = 0.623400       loss = 0.934788\n",
      "Epoch 61, CIFAR-10 Batch 5:  validation accuracy = 0.613400       loss = 0.949443\n",
      "Epoch 62, CIFAR-10 Batch 1:  validation accuracy = 0.620200       loss = 0.963558\n",
      "Epoch 62, CIFAR-10 Batch 2:  validation accuracy = 0.624000       loss = 0.964094\n",
      "Epoch 62, CIFAR-10 Batch 3:  validation accuracy = 0.621000       loss = 0.944444\n",
      "Epoch 62, CIFAR-10 Batch 4:  validation accuracy = 0.619800       loss = 0.938159\n",
      "Epoch 62, CIFAR-10 Batch 5:  validation accuracy = 0.625200       loss = 0.923907\n",
      "Epoch 63, CIFAR-10 Batch 1:  validation accuracy = 0.627800       loss = 0.960841\n",
      "Epoch 63, CIFAR-10 Batch 2:  validation accuracy = 0.622400       loss = 0.960827\n",
      "Epoch 63, CIFAR-10 Batch 3:  validation accuracy = 0.624800       loss = 0.934757\n",
      "Epoch 63, CIFAR-10 Batch 4:  validation accuracy = 0.620600       loss = 0.925196\n",
      "Epoch 63, CIFAR-10 Batch 5:  validation accuracy = 0.625200       loss = 0.930193\n",
      "Epoch 64, CIFAR-10 Batch 1:  validation accuracy = 0.622600       loss = 0.951208\n",
      "Epoch 64, CIFAR-10 Batch 2:  validation accuracy = 0.624400       loss = 0.947609\n",
      "Epoch 64, CIFAR-10 Batch 3:  validation accuracy = 0.625200       loss = 0.916562\n",
      "Epoch 64, CIFAR-10 Batch 4:  validation accuracy = 0.623400       loss = 0.917525\n",
      "Epoch 64, CIFAR-10 Batch 5:  validation accuracy = 0.622600       loss = 0.924529\n",
      "Epoch 65, CIFAR-10 Batch 1:  validation accuracy = 0.623200       loss = 0.952810\n",
      "Epoch 65, CIFAR-10 Batch 2:  validation accuracy = 0.627000       loss = 0.964166\n",
      "Epoch 65, CIFAR-10 Batch 3:  validation accuracy = 0.619200       loss = 0.926395\n",
      "Epoch 65, CIFAR-10 Batch 4:  validation accuracy = 0.625600       loss = 0.916697\n",
      "Epoch 65, CIFAR-10 Batch 5:  validation accuracy = 0.626800       loss = 0.907956\n",
      "Epoch 66, CIFAR-10 Batch 1:  validation accuracy = 0.626200       loss = 0.941209\n",
      "Epoch 66, CIFAR-10 Batch 2:  validation accuracy = 0.617600       loss = 0.960371\n",
      "Epoch 66, CIFAR-10 Batch 3:  validation accuracy = 0.625200       loss = 0.933091\n",
      "Epoch 66, CIFAR-10 Batch 4:  validation accuracy = 0.619400       loss = 0.922664\n",
      "Epoch 66, CIFAR-10 Batch 5:  validation accuracy = 0.628600       loss = 0.917105\n",
      "Epoch 67, CIFAR-10 Batch 1:  validation accuracy = 0.625400       loss = 0.939422\n",
      "Epoch 67, CIFAR-10 Batch 2:  validation accuracy = 0.619000       loss = 0.962601\n",
      "Epoch 67, CIFAR-10 Batch 3:  validation accuracy = 0.621400       loss = 0.922631\n",
      "Epoch 67, CIFAR-10 Batch 4:  validation accuracy = 0.624600       loss = 0.909337\n",
      "Epoch 67, CIFAR-10 Batch 5:  validation accuracy = 0.623600       loss = 0.913794\n",
      "Epoch 68, CIFAR-10 Batch 1:  validation accuracy = 0.630600       loss = 0.938259\n",
      "Epoch 68, CIFAR-10 Batch 2:  validation accuracy = 0.631400       loss = 0.936446\n",
      "Epoch 68, CIFAR-10 Batch 3:  validation accuracy = 0.630000       loss = 0.902746\n",
      "Epoch 68, CIFAR-10 Batch 4:  validation accuracy = 0.618000       loss = 0.913433\n",
      "Epoch 68, CIFAR-10 Batch 5:  validation accuracy = 0.618400       loss = 0.907903\n",
      "Epoch 69, CIFAR-10 Batch 1:  validation accuracy = 0.625000       loss = 0.930518\n",
      "Epoch 69, CIFAR-10 Batch 2:  validation accuracy = 0.626400       loss = 0.930948\n",
      "Epoch 69, CIFAR-10 Batch 3:  validation accuracy = 0.624200       loss = 0.906337\n",
      "Epoch 69, CIFAR-10 Batch 4:  validation accuracy = 0.625400       loss = 0.904189\n",
      "Epoch 69, CIFAR-10 Batch 5:  validation accuracy = 0.626000       loss = 0.894716\n",
      "Epoch 70, CIFAR-10 Batch 1:  validation accuracy = 0.626400       loss = 0.919522\n",
      "Epoch 70, CIFAR-10 Batch 2:  validation accuracy = 0.627400       loss = 0.935114\n",
      "Epoch 70, CIFAR-10 Batch 3:  validation accuracy = 0.627400       loss = 0.902943\n",
      "Epoch 70, CIFAR-10 Batch 4:  validation accuracy = 0.623000       loss = 0.896201\n",
      "Epoch 70, CIFAR-10 Batch 5:  validation accuracy = 0.626800       loss = 0.900242\n",
      "Epoch 71, CIFAR-10 Batch 1:  validation accuracy = 0.627200       loss = 0.923868\n",
      "Epoch 71, CIFAR-10 Batch 2:  validation accuracy = 0.624400       loss = 0.922427\n",
      "Epoch 71, CIFAR-10 Batch 3:  validation accuracy = 0.630800       loss = 0.894591\n",
      "Epoch 71, CIFAR-10 Batch 4:  validation accuracy = 0.631800       loss = 0.890005\n",
      "Epoch 71, CIFAR-10 Batch 5:  validation accuracy = 0.625600       loss = 0.901649\n",
      "Epoch 72, CIFAR-10 Batch 1:  validation accuracy = 0.625600       loss = 0.913154\n",
      "Epoch 72, CIFAR-10 Batch 2:  validation accuracy = 0.630000       loss = 0.920887\n",
      "Epoch 72, CIFAR-10 Batch 3:  validation accuracy = 0.631600       loss = 0.891597\n",
      "Epoch 72, CIFAR-10 Batch 4:  validation accuracy = 0.629400       loss = 0.892005\n",
      "Epoch 72, CIFAR-10 Batch 5:  validation accuracy = 0.627800       loss = 0.893313\n",
      "Epoch 73, CIFAR-10 Batch 1:  validation accuracy = 0.629400       loss = 0.912128\n",
      "Epoch 73, CIFAR-10 Batch 2:  validation accuracy = 0.626000       loss = 0.919785\n",
      "Epoch 73, CIFAR-10 Batch 3:  validation accuracy = 0.633600       loss = 0.895668\n",
      "Epoch 73, CIFAR-10 Batch 4:  validation accuracy = 0.630800       loss = 0.872746\n",
      "Epoch 73, CIFAR-10 Batch 5:  validation accuracy = 0.633000       loss = 0.884716\n",
      "Epoch 74, CIFAR-10 Batch 1:  validation accuracy = 0.629400       loss = 0.921266\n",
      "Epoch 74, CIFAR-10 Batch 2:  validation accuracy = 0.633800       loss = 0.924641\n",
      "Epoch 74, CIFAR-10 Batch 3:  validation accuracy = 0.634800       loss = 0.887585\n",
      "Epoch 74, CIFAR-10 Batch 4:  validation accuracy = 0.629200       loss = 0.885991\n",
      "Epoch 74, CIFAR-10 Batch 5:  validation accuracy = 0.625800       loss = 0.891332\n",
      "Epoch 75, CIFAR-10 Batch 1:  validation accuracy = 0.631800       loss = 0.923845\n",
      "Epoch 75, CIFAR-10 Batch 2:  validation accuracy = 0.625200       loss = 0.917215\n",
      "Epoch 75, CIFAR-10 Batch 3:  validation accuracy = 0.635400       loss = 0.890638\n",
      "Epoch 75, CIFAR-10 Batch 4:  validation accuracy = 0.627600       loss = 0.886233\n",
      "Epoch 75, CIFAR-10 Batch 5:  validation accuracy = 0.627800       loss = 0.890064\n",
      "Epoch 76, CIFAR-10 Batch 1:  validation accuracy = 0.629800       loss = 0.901836\n",
      "Epoch 76, CIFAR-10 Batch 2:  validation accuracy = 0.629400       loss = 0.915410\n",
      "Epoch 76, CIFAR-10 Batch 3:  validation accuracy = 0.638000       loss = 0.877344\n",
      "Epoch 76, CIFAR-10 Batch 4:  validation accuracy = 0.629800       loss = 0.871182\n",
      "Epoch 76, CIFAR-10 Batch 5:  validation accuracy = 0.630200       loss = 0.876636\n",
      "Epoch 77, CIFAR-10 Batch 1:  validation accuracy = 0.635600       loss = 0.894361\n",
      "Epoch 77, CIFAR-10 Batch 2:  validation accuracy = 0.624400       loss = 0.925295\n",
      "Epoch 77, CIFAR-10 Batch 3:  validation accuracy = 0.631600       loss = 0.872876\n",
      "Epoch 77, CIFAR-10 Batch 4:  validation accuracy = 0.632800       loss = 0.862481\n",
      "Epoch 77, CIFAR-10 Batch 5:  validation accuracy = 0.637400       loss = 0.869085\n",
      "Epoch 78, CIFAR-10 Batch 1:  validation accuracy = 0.633400       loss = 0.896941\n",
      "Epoch 78, CIFAR-10 Batch 2:  validation accuracy = 0.639200       loss = 0.892933\n",
      "Epoch 78, CIFAR-10 Batch 3:  validation accuracy = 0.637600       loss = 0.865228\n",
      "Epoch 78, CIFAR-10 Batch 4:  validation accuracy = 0.639000       loss = 0.856538\n",
      "Epoch 78, CIFAR-10 Batch 5:  validation accuracy = 0.629600       loss = 0.871058\n",
      "Epoch 79, CIFAR-10 Batch 1:  validation accuracy = 0.637000       loss = 0.887331\n",
      "Epoch 79, CIFAR-10 Batch 2:  validation accuracy = 0.631800       loss = 0.902444\n",
      "Epoch 79, CIFAR-10 Batch 3:  validation accuracy = 0.632000       loss = 0.879014\n",
      "Epoch 79, CIFAR-10 Batch 4:  validation accuracy = 0.627800       loss = 0.861234\n",
      "Epoch 79, CIFAR-10 Batch 5:  validation accuracy = 0.635800       loss = 0.874627\n",
      "Epoch 80, CIFAR-10 Batch 1:  validation accuracy = 0.642600       loss = 0.880008\n",
      "Epoch 80, CIFAR-10 Batch 2:  validation accuracy = 0.634000       loss = 0.903326\n",
      "Epoch 80, CIFAR-10 Batch 3:  validation accuracy = 0.629400       loss = 0.870843\n",
      "Epoch 80, CIFAR-10 Batch 4:  validation accuracy = 0.629600       loss = 0.855357\n",
      "Epoch 80, CIFAR-10 Batch 5:  validation accuracy = 0.630600       loss = 0.855007\n",
      "Epoch 81, CIFAR-10 Batch 1:  validation accuracy = 0.636800       loss = 0.889679\n",
      "Epoch 81, CIFAR-10 Batch 2:  validation accuracy = 0.632600       loss = 0.897583\n",
      "Epoch 81, CIFAR-10 Batch 3:  validation accuracy = 0.635800       loss = 0.871017\n",
      "Epoch 81, CIFAR-10 Batch 4:  validation accuracy = 0.633000       loss = 0.852456\n",
      "Epoch 81, CIFAR-10 Batch 5:  validation accuracy = 0.633800       loss = 0.871881\n",
      "Epoch 82, CIFAR-10 Batch 1:  validation accuracy = 0.639200       loss = 0.901376\n",
      "Epoch 82, CIFAR-10 Batch 2:  validation accuracy = 0.636800       loss = 0.887864\n",
      "Epoch 82, CIFAR-10 Batch 3:  validation accuracy = 0.637400       loss = 0.859019\n",
      "Epoch 82, CIFAR-10 Batch 4:  validation accuracy = 0.631600       loss = 0.852170\n",
      "Epoch 82, CIFAR-10 Batch 5:  validation accuracy = 0.629600       loss = 0.864133\n",
      "Epoch 83, CIFAR-10 Batch 1:  validation accuracy = 0.627800       loss = 0.889196\n",
      "Epoch 83, CIFAR-10 Batch 2:  validation accuracy = 0.639000       loss = 0.889859\n",
      "Epoch 83, CIFAR-10 Batch 3:  validation accuracy = 0.634800       loss = 0.861745\n",
      "Epoch 83, CIFAR-10 Batch 4:  validation accuracy = 0.637400       loss = 0.850309\n",
      "Epoch 83, CIFAR-10 Batch 5:  validation accuracy = 0.635600       loss = 0.861167\n",
      "Epoch 84, CIFAR-10 Batch 1:  validation accuracy = 0.638600       loss = 0.872730\n",
      "Epoch 84, CIFAR-10 Batch 2:  validation accuracy = 0.636600       loss = 0.891814\n",
      "Epoch 84, CIFAR-10 Batch 3:  validation accuracy = 0.639200       loss = 0.848224\n",
      "Epoch 84, CIFAR-10 Batch 4:  validation accuracy = 0.637400       loss = 0.840673\n",
      "Epoch 84, CIFAR-10 Batch 5:  validation accuracy = 0.635400       loss = 0.856948\n",
      "Epoch 85, CIFAR-10 Batch 1:  validation accuracy = 0.636200       loss = 0.873863\n",
      "Epoch 85, CIFAR-10 Batch 2:  validation accuracy = 0.636000       loss = 0.891337\n",
      "Epoch 85, CIFAR-10 Batch 3:  validation accuracy = 0.641200       loss = 0.853418\n",
      "Epoch 85, CIFAR-10 Batch 4:  validation accuracy = 0.639000       loss = 0.827477\n",
      "Epoch 85, CIFAR-10 Batch 5:  validation accuracy = 0.632200       loss = 0.849457\n",
      "Epoch 86, CIFAR-10 Batch 1:  validation accuracy = 0.640200       loss = 0.866645\n",
      "Epoch 86, CIFAR-10 Batch 2:  validation accuracy = 0.635400       loss = 0.863773\n",
      "Epoch 86, CIFAR-10 Batch 3:  validation accuracy = 0.637000       loss = 0.847226\n",
      "Epoch 86, CIFAR-10 Batch 4:  validation accuracy = 0.635400       loss = 0.838077\n",
      "Epoch 86, CIFAR-10 Batch 5:  validation accuracy = 0.633000       loss = 0.852897\n",
      "Epoch 87, CIFAR-10 Batch 1:  validation accuracy = 0.643800       loss = 0.873886\n",
      "Epoch 87, CIFAR-10 Batch 2:  validation accuracy = 0.640000       loss = 0.875001\n",
      "Epoch 87, CIFAR-10 Batch 3:  validation accuracy = 0.641400       loss = 0.836397\n",
      "Epoch 87, CIFAR-10 Batch 4:  validation accuracy = 0.636800       loss = 0.831176\n",
      "Epoch 87, CIFAR-10 Batch 5:  validation accuracy = 0.646600       loss = 0.838864\n",
      "Epoch 88, CIFAR-10 Batch 1:  validation accuracy = 0.632000       loss = 0.870094\n",
      "Epoch 88, CIFAR-10 Batch 2:  validation accuracy = 0.637200       loss = 0.875515\n",
      "Epoch 88, CIFAR-10 Batch 3:  validation accuracy = 0.640200       loss = 0.857323\n",
      "Epoch 88, CIFAR-10 Batch 4:  validation accuracy = 0.639000       loss = 0.831516\n",
      "Epoch 88, CIFAR-10 Batch 5:  validation accuracy = 0.644400       loss = 0.857129\n",
      "Epoch 89, CIFAR-10 Batch 1:  validation accuracy = 0.637800       loss = 0.887896\n",
      "Epoch 89, CIFAR-10 Batch 2:  validation accuracy = 0.633400       loss = 0.860844\n",
      "Epoch 89, CIFAR-10 Batch 3:  validation accuracy = 0.640400       loss = 0.841341\n",
      "Epoch 89, CIFAR-10 Batch 4:  validation accuracy = 0.628800       loss = 0.823693\n",
      "Epoch 89, CIFAR-10 Batch 5:  validation accuracy = 0.634400       loss = 0.844177\n",
      "Epoch 90, CIFAR-10 Batch 1:  validation accuracy = 0.635200       loss = 0.853615\n",
      "Epoch 90, CIFAR-10 Batch 2:  validation accuracy = 0.636000       loss = 0.872617\n",
      "Epoch 90, CIFAR-10 Batch 3:  validation accuracy = 0.639800       loss = 0.842997\n",
      "Epoch 90, CIFAR-10 Batch 4:  validation accuracy = 0.635200       loss = 0.829500\n",
      "Epoch 90, CIFAR-10 Batch 5:  validation accuracy = 0.634000       loss = 0.849483\n",
      "Epoch 91, CIFAR-10 Batch 1:  validation accuracy = 0.635800       loss = 0.859811\n",
      "Epoch 91, CIFAR-10 Batch 2:  validation accuracy = 0.637800       loss = 0.861860\n",
      "Epoch 91, CIFAR-10 Batch 3:  validation accuracy = 0.643000       loss = 0.835962\n",
      "Epoch 91, CIFAR-10 Batch 4:  validation accuracy = 0.631800       loss = 0.823149\n",
      "Epoch 91, CIFAR-10 Batch 5:  validation accuracy = 0.641200       loss = 0.846610\n",
      "Epoch 92, CIFAR-10 Batch 1:  validation accuracy = 0.635600       loss = 0.864559\n",
      "Epoch 92, CIFAR-10 Batch 2:  validation accuracy = 0.635800       loss = 0.854043\n",
      "Epoch 92, CIFAR-10 Batch 3:  validation accuracy = 0.643000       loss = 0.823353\n",
      "Epoch 92, CIFAR-10 Batch 4:  validation accuracy = 0.637200       loss = 0.823875\n",
      "Epoch 92, CIFAR-10 Batch 5:  validation accuracy = 0.633400       loss = 0.832337\n",
      "Epoch 93, CIFAR-10 Batch 1:  validation accuracy = 0.634600       loss = 0.858450\n",
      "Epoch 93, CIFAR-10 Batch 2:  validation accuracy = 0.643400       loss = 0.850287\n",
      "Epoch 93, CIFAR-10 Batch 3:  validation accuracy = 0.646600       loss = 0.817506\n",
      "Epoch 93, CIFAR-10 Batch 4:  validation accuracy = 0.631800       loss = 0.814670\n",
      "Epoch 93, CIFAR-10 Batch 5:  validation accuracy = 0.634400       loss = 0.840094\n",
      "Epoch 94, CIFAR-10 Batch 1:  validation accuracy = 0.645200       loss = 0.836456\n",
      "Epoch 94, CIFAR-10 Batch 2:  validation accuracy = 0.642600       loss = 0.866244\n",
      "Epoch 94, CIFAR-10 Batch 3:  validation accuracy = 0.642800       loss = 0.821487\n",
      "Epoch 94, CIFAR-10 Batch 4:  validation accuracy = 0.631200       loss = 0.819945\n",
      "Epoch 94, CIFAR-10 Batch 5:  validation accuracy = 0.627400       loss = 0.846620\n",
      "Epoch 95, CIFAR-10 Batch 1:  validation accuracy = 0.634200       loss = 0.869315\n",
      "Epoch 95, CIFAR-10 Batch 2:  validation accuracy = 0.642000       loss = 0.864261\n",
      "Epoch 95, CIFAR-10 Batch 3:  validation accuracy = 0.647600       loss = 0.817478\n",
      "Epoch 95, CIFAR-10 Batch 4:  validation accuracy = 0.638400       loss = 0.805877\n",
      "Epoch 95, CIFAR-10 Batch 5:  validation accuracy = 0.628400       loss = 0.860247\n",
      "Epoch 96, CIFAR-10 Batch 1:  validation accuracy = 0.641800       loss = 0.849654\n",
      "Epoch 96, CIFAR-10 Batch 2:  validation accuracy = 0.634800       loss = 0.855614\n",
      "Epoch 96, CIFAR-10 Batch 3:  validation accuracy = 0.643200       loss = 0.824218\n",
      "Epoch 96, CIFAR-10 Batch 4:  validation accuracy = 0.644200       loss = 0.823360\n",
      "Epoch 96, CIFAR-10 Batch 5:  validation accuracy = 0.633000       loss = 0.847857\n",
      "Epoch 97, CIFAR-10 Batch 1:  validation accuracy = 0.639000       loss = 0.842610\n",
      "Epoch 97, CIFAR-10 Batch 2:  validation accuracy = 0.648400       loss = 0.838223\n",
      "Epoch 97, CIFAR-10 Batch 3:  validation accuracy = 0.643200       loss = 0.810813\n",
      "Epoch 97, CIFAR-10 Batch 4:  validation accuracy = 0.642200       loss = 0.801584\n",
      "Epoch 97, CIFAR-10 Batch 5:  validation accuracy = 0.636600       loss = 0.837723\n",
      "Epoch 98, CIFAR-10 Batch 1:  validation accuracy = 0.638200       loss = 0.842436\n",
      "Epoch 98, CIFAR-10 Batch 2:  validation accuracy = 0.639000       loss = 0.846565\n",
      "Epoch 98, CIFAR-10 Batch 3:  validation accuracy = 0.642400       loss = 0.811084\n",
      "Epoch 98, CIFAR-10 Batch 4:  validation accuracy = 0.642000       loss = 0.798101\n",
      "Epoch 98, CIFAR-10 Batch 5:  validation accuracy = 0.643800       loss = 0.833565\n",
      "Epoch 99, CIFAR-10 Batch 1:  validation accuracy = 0.636000       loss = 0.836267\n",
      "Epoch 99, CIFAR-10 Batch 2:  validation accuracy = 0.642400       loss = 0.830177\n",
      "Epoch 99, CIFAR-10 Batch 3:  validation accuracy = 0.642200       loss = 0.815797\n",
      "Epoch 99, CIFAR-10 Batch 4:  validation accuracy = 0.646600       loss = 0.795310\n",
      "Epoch 99, CIFAR-10 Batch 5:  validation accuracy = 0.643600       loss = 0.806033\n",
      "Epoch 100, CIFAR-10 Batch 1:  validation accuracy = 0.639600       loss = 0.827237\n",
      "Epoch 100, CIFAR-10 Batch 2:  validation accuracy = 0.648200       loss = 0.827856\n",
      "Epoch 100, CIFAR-10 Batch 3:  validation accuracy = 0.645400       loss = 0.800382\n",
      "Epoch 100, CIFAR-10 Batch 4:  validation accuracy = 0.641200       loss = 0.783906\n",
      "Epoch 100, CIFAR-10 Batch 5:  validation accuracy = 0.638800       loss = 0.812214\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.6335220038890839\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n5FhEREREJNLkWEREREQk+h+7IqGLVBCu7QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x147da7fd0>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for train_feature_batch, train_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why 50-70% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 70%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
